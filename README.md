# Whisper Analyzer

Kompleksowy system do transkrypcji i analizy tre≈õci plik√≥w audio zgodnie z PRD.

## Funkcjonalno≈õci

### Faza 1 (Obecna)
- ‚úÖ Transkrypcja plik√≥w MP3 za pomocƒÖ Whisper (model large-v3)
- ‚úÖ **Rozpoznawanie m√≥wc√≥w (Speaker Diarization)** - nowo≈õƒá!
- ‚úÖ Obs≈Çuga jƒôzyka polskiego z dok≈Çadno≈õciƒÖ do 98%
- ‚úÖ Automatyczne monitorowanie folderu wej≈õciowego
- ‚úÖ R√≥wnoleg≈Çe przetwarzanie (max 4 pliki jednocze≈õnie)
- ‚úÖ Szyfrowanie plik√≥w tymczasowych (AES-256)
- ‚úÖ Obs≈Çuga b≈Çƒôd√≥w z automatycznymi ponownymi pr√≥bami
- ‚úÖ Logowanie wszystkich operacji

### Faza 2 (Planowana)
- üîÑ Integracja z Ollama do analizy tre≈õci
- üîÑ Wykrywanie tre≈õci wulgarnych/agresywnych
- üîÑ Nazewnictwo wynik√≥w z oznaczeniem analizy

## Rozpoznawanie M√≥wc√≥w üéØ

Aplikacja automatycznie rozpoznaje i rozdziela r√≥≈ºnych m√≥wc√≥w w nagraniu:

### Przyk≈Çad wyj≈õcia:
```
[00:00-00:03] SPEAKER_00: Dzie≈Ñ dobry, dzwoniƒô w sprawie mojego zam√≥wienia
[00:04-00:08] SPEAKER_01: Dzie≈Ñ dobry, jak mogƒô pom√≥c?
[00:09-00:15] SPEAKER_00: Mam problem z moim zam√≥wieniem numer 12345
```

### Generowane pliki:
- `nazwa_pliku.txt` - Standardowa transkrypcja
- `nazwa_pliku_with_speakers.txt` - Transkrypcja z adnotacjami m√≥wc√≥w
- `nazwa_pliku_metadata.json` - Metadane w formacie JSON

**Idealne dla call center!** Rozr√≥≈ºnia doradc√≥w klienta od klient√≥w.

## Wymagania systemowe

- Python 3.10+
- Linux (Ubuntu 22.04+) lub Windows 11 z WSL2
- FFmpeg
- Minimum 8GB RAM (16GB zalecane)
- GPU z CUDA (opcjonalne, ale zalecane dla szybszego przetwarzania)

## Instalacja

1. **Klonowanie repozytorium**
```bash
git clone <repository-url>
cd Whisper
```

2. **Utworzenie ≈õrodowiska wirtualnego**
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# lub
.venv\Scripts\activate     # Windows
```

3. **Konfiguracja zmiennych ≈õrodowiskowych**
```bash
cp .env.example .env
# Uzupe≈Çnij .env (token HuggingFace, adres serwera Ollama itd.)
```

4. **Instalacja zale≈ºno≈õci**
```bash
pip install -r requirements.txt
```

5. **Instalacja FFmpeg**
```bash
# Ubuntu/Debian
sudo apt update && sudo apt install -y ffmpeg

# CentOS/RHEL
sudo yum install ffmpeg

# macOS
brew install ffmpeg
```

5. **Konfiguracja rozpoznawania m√≥wc√≥w (opcjonalne)**
```bash
# Dla najlepszej wydajno≈õci, uzyskaj token na:
# https://huggingface.co/pyannote/speaker-diarization-3.1
```

## U≈ºytkowanie

### Szybki start

1. **Uruchomienie aplikacji**
   ```bash
   # Automatyczne uruchomienie (Ubuntu / WSL)
   ./run.sh

   # Rƒôczna aktywacja
   source .venv/bin/activate
   python main.py
   ```

   Skrypt `run.sh` automatycznie tworzy/aktywuje ≈õrodowisko `.venv`, instaluje zale≈ºno≈õci i uruchamia aplikacjƒô.

2. **Umieszczenie plik√≥w audio**
- Umie≈õƒá pliki MP3 w folderze `input/`
- Aplikacja automatycznie wykryje i przetworzy nowe pliki
- Wyniki transkrypcji zostanƒÖ zapisane w folderze `output/`
- Modele Whisper pobierajƒÖ siƒô automatycznie do podfolderu `models/` w katalogu projektu
- Token Hugging Face (`SPEAKER_DIARIZATION_TOKEN`) wpisz w pliku `.env` po zaakceptowaniu licencji na https://huggingface.co/pyannote/speaker-diarization-3.1

### Struktura folder√≥w

```
Whisper/
‚îú‚îÄ‚îÄ input/           # Folder z plikami MP3 do przetworzenia
‚îú‚îÄ‚îÄ output/          # Folder z wynikami transkrypcji
‚îÇ   ‚îú‚îÄ‚îÄ rozmowa_01.txt                    # Standardowa transkrypcja
‚îÇ   ‚îú‚îÄ‚îÄ rozmowa_01_with_speakers.txt      # Transkrypcja z m√≥wcami
‚îÇ   ‚îî‚îÄ‚îÄ rozmowa_01_metadata.json          # Metadane JSON
‚îú‚îÄ‚îÄ models/          # Lokalna pamiƒôƒá podrƒôczna modeli Whisper
‚îú‚îÄ‚îÄ .env.example     # Szablon zmiennych ≈õrodowiskowych
‚îú‚îÄ‚îÄ .env             # Lokalna konfiguracja (nie trafia do repozytorium)
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ whisper_analyzer.py
‚îú‚îÄ‚îÄ run.sh
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ SPEAKER_DIARIZATION.md               # Dokumentacja rozpoznawania m√≥wc√≥w
‚îî‚îÄ‚îÄ whisper_analyzer.log  # Plik log√≥w
```

### Przyk≈Çad u≈ºycia

```bash
# 1. Uruchom aplikacjƒô
python whisper_analyzer.py

# 2. W nowym terminalu skopiuj plik audio
cp /≈õcie≈ºka/do/rozmowy.mp3 input/

# 3. Sprawd≈∫ wyniki w folderze output/
ls output/
cat output/rozmowy_with_speakers.txt
```

## Konfiguracja

### Parametry aplikacji

Mo≈ºesz dostosowaƒá parametry w kodzie:

```python
# W klasie AudioProcessor
self.max_concurrent = 4  # Maksymalna liczba r√≥wnoczesnych przetwarza≈Ñ
self.input_folder = "input"  # Folder wej≈õciowy
self.output_folder = "output"  # Folder wyj≈õciowy
self.enable_speaker_diarization = True  # W≈ÇƒÖcz/wy≈ÇƒÖcz rozpoznawanie m√≥wc√≥w
```

#### ≈öcie≈ºki modeli i urzƒÖdzenia

- Modele Whisper sƒÖ buforowane w `models/` wzglƒôdnie do katalogu projektu (zmienna `MODEL_CACHE_DIR`).
- Aplikacja automatycznie wykrywa dostƒôpno≈õƒá GPU; przy braku akceleratora przechodzi na CPU i wymusza transkrypcjƒô w trybie `fp16=False`.
- Do rozpoznawania m√≥wc√≥w wymagany jest token Hugging Face (`SPEAKER_DIARIZATION_TOKEN`) uzyskany po zaakceptowaniu warunk√≥w repozytorium https://huggingface.co/pyannote/speaker-diarization-3.1.

### Model Whisper

Domy≈õlnie u≈ºywany jest model `large-v3` dla najwy≈ºszej dok≈Çadno≈õci. Mo≈ºesz zmieniƒá na:

- `tiny` - najszybszy, najmniej dok≈Çadny
- `base` - szybki, podstawowa dok≈Çadno≈õƒá
- `small` - ≈õrednia prƒôdko≈õƒá i dok≈Çadno≈õƒá
- `medium` - wolniejszy, wy≈ºsza dok≈Çadno≈õƒá
- `large-v3` - najwolniejszy, najwy≈ºsza dok≈Çadno≈õƒá

### Rozpoznawanie m√≥wc√≥w

Aby w≈ÇƒÖczyƒá rozpoznawanie m√≥wc√≥w z tokenem HuggingFace:

```python
# W funkcji main()
auth_token = "hf_your_token_here"
processor.initialize_speaker_diarization(auth_token)
```

## Monitorowanie

### Logi

Aplikacja generuje szczeg√≥≈Çowe logi w pliku `whisper_analyzer.log`:

```
2025-01-XX 10:30:15 - INFO - === Uruchamianie aplikacji Whisper Analyzer ===
2025-01-XX 10:30:16 - INFO - ≈Åadowanie modelu Whisper: large-v3
2025-01-XX 10:30:45 - INFO - Model Whisper za≈Çadowany pomy≈õlnie
2025-01-XX 10:30:46 - INFO - Rozpoznawanie m√≥wc√≥w: W≈ÇƒÖczone
2025-01-XX 10:30:47 - INFO - Rozpoznawanie m√≥wc√≥w zainicjalizowane pomy≈õlnie
2025-01-XX 10:30:48 - INFO - Znaleziono 2 plik√≥w MP3 do przetworzenia
2025-01-XX 10:30:49 - INFO - Transkrypcja pliku: rozmowa_01.mp3 (pr√≥ba 1/3)
2025-01-XX 10:32:15 - INFO - Rozpoznano 2 m√≥wc√≥w
2025-01-XX 10:32:16 - INFO - Transkrypcja zako≈Ñczona pomy≈õlnie: rozmowa_01.mp3
```

### Wydajno≈õƒá

Typowe czasy przetwarzania (na CPU):
- **Transkrypcja:** Model `large-v3`: ~5 min/30 min nagrania
- **Rozpoznawanie m√≥wc√≥w:** ~10-15 min/30 min nagrania
- **Ca≈Çkowity czas:** ~15-20 min/30 min nagrania

Z GPU CUDA czasy mogƒÖ byƒá 3-5x szybsze.

### Dok≈Çadno≈õƒá rozpoznawania m√≥wc√≥w:
- **Rozpoznawanie liczby m√≥wc√≥w:** 95%+
- **Rozdzielenie wypowiedzi:** 90%+
- **Dok≈Çadno≈õƒá czasowa:** ¬±0.5 sekundy

## Bezpiecze≈Ñstwo

- Pliki tymczasowe sƒÖ szyfrowane za pomocƒÖ AES-256
- Brak wysy≈Çania danych do chmury - wszystko przetwarzane lokalnie
- Automatyczne usuwanie plik√≥w tymczasowych
- Walidacja plik√≥w wej≈õciowych

## RozwiƒÖzywanie problem√≥w

### B≈ÇƒÖd: "Model Whisper nie zosta≈Ç za≈Çadowany"
```bash
# Sprawd≈∫ czy model zosta≈Ç pobrany
python -c "import whisper; whisper.load_model('large-v3')"
```

### B≈ÇƒÖd: "FFmpeg not found"
```bash
# Zainstaluj FFmpeg
sudo apt install ffmpeg
```

### B≈ÇƒÖd: "pyannote.audio nie jest dostƒôpne"
```bash
# Zainstaluj zale≈ºno≈õci rozpoznawania m√≥wc√≥w
pip install pyannote.audio torch torchaudio librosa soundfile
```

### B≈ÇƒÖd: "Out of memory"
- Zmniejsz `max_concurrent` w kodzie
- U≈ºyj mniejszego modelu Whisper
- Zamknij inne aplikacje zu≈ºywajƒÖce du≈ºo RAM

### Wolne przetwarzanie
- Sprawd≈∫ czy masz GPU z CUDA
- U≈ºyj mniejszego modelu Whisper
- Sprawd≈∫ czy nie ma innych proces√≥w zu≈ºywajƒÖcych CPU

### Niskie rozpoznawanie m√≥wc√≥w
- Sprawd≈∫ jako≈õƒá audio (szum, echo)
- Upewnij siƒô, ≈ºe m√≥wcy nie nak≈ÇadajƒÖ siƒô na siebie
- Sprawd≈∫ czy audio ma odpowiedniƒÖ d≈Çugo≈õƒá (>10 sekund)

## Analiza wynik√≥w

### Przyk≈Çad analizy JSON:
```python
import json

with open('rozmowa_01_metadata.json', 'r') as f:
    data = json.load(f)

# Liczba m√≥wc√≥w
speakers = set([s['speaker'] for s in data['speakers']])
print(f"Liczba m√≥wc√≥w: {len(speakers)}")

# Czas m√≥wienia ka≈ºdej osoby
speaker_times = {}
for speaker_info in data['speakers']:
    speaker = speaker_info['speaker']
    if speaker not in speaker_times:
        speaker_times[speaker] = 0
    speaker_times[speaker] += speaker_info['duration']

for speaker, time in speaker_times.items():
    print(f"{speaker}: {time:.1f} sekund")
```

## Rozw√≥j

### Dodanie nowych funkcjonalno≈õci

1. Edytuj `whisper_analyzer.py`
2. Dodaj nowe zale≈ºno≈õci do `requirements.txt`
3. Przetestuj zmiany
4. Zaktualizuj dokumentacjƒô

### Testowanie

```bash
# Test podstawowej funkcjonalno≈õci
python -c "from whisper_analyzer import AudioProcessor; p = AudioProcessor(); print('OK')"

# Test rozpoznawania m√≥wc√≥w
python -c "from whisper_analyzer import SpeakerDiarizer; s = SpeakerDiarizer(); print('OK')"
```

## Dokumentacja

- [Rozpoznawanie M√≥wc√≥w](SPEAKER_DIARIZATION.md) - Szczeg√≥≈Çowa dokumentacja funkcji rozpoznawania m√≥wc√≥w
- [PRD](PRD.txt) - Dokument wymaga≈Ñ aplikacji

## Licencja

Projekt zgodny z wymaganiami PRD.

## Wsparcie

W przypadku problem√≥w sprawd≈∫:
1. Logi w `whisper_analyzer.log`
2. Dokumentacjƒô Whisper: https://github.com/openai/whisper
3. Dokumentacjƒô pyannote.audio: https://github.com/pyannote/pyannote-audio
4. Wymagania systemowe 