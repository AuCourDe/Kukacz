SPEAKER_DIARIZATION_TOKEN=hf_your_token_here  # alternatywy: "" (wyłącza pyannote) – wpływa na dokładność rozpoznawania mówców
WHISPER_MODEL=base  # alternatywy: small (szybsze na CPU), large-v3 (maksymalna jakość kosztem zasobów) – wpływa na jakość i czas transkrypcji
OLLAMA_MODEL=qwen3:8b  # alternatywy: qwen3:14b (dokładniejsze, większe zużycie), llama3.1:8b (szybsze) – wpływa na szczegółowość analizy treści
OLLAMA_BASE_URL=http://localhost:11434  # alternatywy: http://<remote-ip>:11434 (zdalny serwer) – wpływa na adres API Ollama
OLLAMA_SYSTEM_PROMPT="You are a security-hardened call analysis engine."  # alternatywy: własny prompt bezpieczeństwa – wpływa na styl i zasady generowanych analiz
MAX_TRANSCRIPT_LENGTH=8000  # alternatywy: 4000 (mniej tekstu, szybsze), 12000 (więcej tekstu, ryzyko timeout) – wpływa na długość przekazywanej transkrypcji
PROMPT_DIR=prompt  # alternatywy: custom_prompts – wpływa na katalog z plikami promptów
PROMPT_FILE=prompt.txt  # alternatywy: call_center_prompt.txt – wpływa na domyślny prompt analizy treści
OLLAMA_TEMPERATURE=0.7  # alternatywy: 0.3 (bardziej deterministyczne), 1.0 (bardziej kreatywne) – wpływa na losowość generacji
OLLAMA_TOP_P=0.9  # alternatywy: 0.8 (bardziej konserwatywne), 1.0 (pełna dystrybucja) – wpływa na zakres próbkowania tokenów
OLLAMA_TOP_K=40  # alternatywy: 20 (szybciej, mniej różnorodnie), 80 (wolniej, większa różnorodność) – wpływa na liczbę rozważanych tokenów
OLLAMA_REPEAT_PENALTY=1.1  # alternatywy: 1.0 (bez kary powtórzeń), 1.3 (silniejsze karanie) – wpływa na powtarzalność odpowiedzi
OLLAMA_MAX_TOKENS=2048  # alternatywy: 1024 (krótsze odpowiedzi), 4096 (dłuższe odpowiedzi) – wpływa na długość wyników analizy
OLLAMA_CONNECT_TIMEOUT=10.0  # alternatywy: 5.0 (krótszy czas oczekiwania), 20.0 (dłuższy) – wpływa na limit czasu na zestawienie połączenia
OLLAMA_REQUEST_TIMEOUT=60.0  # alternatywy: 45.0 (ostrzejszy timeout), 120.0 (łagodniejszy) – wpływa na maksymalny czas oczekiwania na odpowiedź
OLLAMA_DEBUG_LOGGING=false  # alternatywy: true (pełne logi request/response) – wpływa na poziom diagnostyki zapytań Ollama
OLLAMA_STREAM_RESPONSES=false  # alternatywy: true (odbiór strumieniowy z logiem chunków) – wpływa na sposób odbierania odpowiedzi
OLLAMA_PROMPT_LOG_MAX_CHARS=2000  # alternatywy: 0 (bez podglądu), 500 (krótki podgląd) – wpływa na długość logowanego promptu
OLLAMA_STREAM_LOG_CHUNK_LIMIT=200  # alternatywy: 50 (krótkie logi), 0 (wyłącza log chunków) – wpływa na rozmiar logowanych fragmentów strumienia
INPUT_FOLDER=input  # alternatywy: MEDIA_FILES (praca bezpośrednio na katalogu produkcyjnym) – wpływa na lokalizację plików wejściowych
OUTPUT_FOLDER=output  # alternatywy: reports (inny katalog wyników) – wpływa na miejsce zapisu transkrypcji i analiz
PROCESSED_FOLDER=processed  # alternatywy: archive/processed (współdzielone archiwum) – wpływa na lokalizację przenoszonych plików audio
MODEL_CACHE_DIR=models  # alternatywy: /mnt/cache/models – wpływa na lokalizację modeli Whisper
ENABLE_SPEAKER_DIARIZATION=true  # alternatywy: false (wyłącza rozpoznawanie mówców) – wpływa na dostępność statystyk mówców
ENABLE_OLLAMA_ANALYSIS=true  # alternatywy: false (pomija analizy treści) – wpływa na generowanie raportów z Ollama
MAX_CONCURRENT_PROCESSES=1  # alternatywy: 2 (większa szybkość), 4 (agresywna równoległość) – wpływa na liczbę równoczesnych przetwarzań
LOG_LEVEL=INFO  # alternatywy: DEBUG (więcej logów), WARNING (mniej logów) – wpływa na szczegółowość logów
LOG_FILE=whisper_analyzer.log  # alternatywy: logs/whisper.log – wpływa na lokalizację pliku logów
MAX_RETRIES=3  # alternatywy: 1 (mniej prób), 5 (więcej prób) – wpływa na odporność transkrypcji na błędy
RETRY_DELAY_BASE=2  # alternatywy: 1 (krótsze odstępy), 4 (dłuższe odstępy) – wpływa na tempo ponowień transkrypcji
ENABLE_FILE_ENCRYPTION=true  # alternatywy: false (bez szyfrowania) – wpływa na bezpieczeństwo plików tymczasowych
TEMPORARY_FILE_CLEANUP=true  # alternatywy: false (zostawia pliki do debugowania) – wpływa na czystość katalogów tymczasowych
APP_RUN_ONCE=false  # alternatywy: true (aplikacja kończy się po jednym przebiegu) – wpływa na tryb pracy głównej pętli

